# Bölüm 9: Ollama - Lokal Yapay Zeka Devriminin Motoru
Yapay zeka uygulamaları geliştirirken karşılaştığımız en temel ikilemlerden biri şudur: En güçlü modellere erişim için verilerimizi bir bulut API'sine mi göndermeliyiz, yoksa tam veri kontrolü ve gizlilik için kendi altyapımızı kurmanın karmaşıklığıyla mı boğuşmalıyız?

Yakın zamana kadar, ikinci seçenek (modelleri yerelde çalıştırmak), ciddi bir mühendislik yükü anlamına geliyordu: Python ortamları, CUDA sürücü uyumsuzlukları, `bitsandbytes` ile kuantizasyon (quantization) ayarları ve karmaşık model formatları (GGUF, GGML) arasında kaybolmak demekti.

**Ollama**, işte bu karmaşıklığı ortadan kaldıran bir "soyutlama" (abstraction) katmanıdır.

Ollama'nın asıl dehası, yeni modeller icat etmek değil; Llama 3, Mistral veya Gemma gibi mevcut en iyi açık kaynaklı modelleri alıp, bunları kişisel bilgisayarlarda (laptop, desktop) çalıştırmayı `ollama run llama3` komutu kadar basit bir hale getirmesidir.

## 1. Ollama Nasıl Çalışır? Mimarinin Özü
Ollama, Python tabanlı bir kütüphaneden daha fazlasıdır; bilgisayarınızda arka planda çalışan (bir **daemon veya servis olarak**) optimize edilmiş bir **Go programıdır**. Bu servis, iki ana görevi yerine getirir:

1.  **Model Yönetimi:** Modelleri indirir, optimize eder (özellikle GGUF gibi formatları kullanarak) ve yerel donanımınızda (Apple M serisi çiplerin GPU'su veya NVIDIA GPU'ları) en verimli şekilde çalışacak hale getirir.
2.  **API Sunucusu:** `http://localhost:11434` adresinde, dış dünyayla konuşan bir REST API sunucusu ayağa kaldırır.

Bu mimarinin en parlak yanı, bu yerel API'nin **OpenAI API spesifikasyonuyla** büyük ölçüde uyumlu olmasıdır.

*Bu ne anlama geliyor? Geliştiricinin, `langchain` veya `llama-index` gibi popüler kütüphanelerde `base_url` parametresini OpenAI'nin adresi yerine kendi yerel adresine (`localhost:11434`) yönlendirmesi yeterlidir. Bu küçük değişiklik, bulut tabanlı bir uygulamayı, kodun geri kalanına dokunmadan, %100 yerel, çevrimdışı ve özel bir uygulamaya dönüştürür.*

## 2. Modelfile: Modeller için Dockerfile
Ollama'nın bir diğer güçlü konsepti ise `Modelfile` yapısıdır. Tıpkı bir `Dockerfile`'ın bir uygulamanın nasıl paketleneceğini tanımlaması gibi, `Modelfile` da bir temel modelin (base model) nasıl özelleştirileceğini tanımlar.

Bir `Modelfile` kullanarak:
* Modelin **sistem mesajını (system prompt)** kalıcı olarak ayarlayabilirsiniz (örn. "Sen, yalnızca JSON formatında cevap veren bir asistansın.").
* Parametreleri (sıcaklık `temperature` gibi) önceden tanımlayabilirsiniz.
* Modeli (örn. Llama 3 8B) temel alıp, üzerine kendi kurallarınızı ekleyerek `my-custom-model` adında yeni bir yerel model oluşturabilirsiniz.

Bu, test ve geliştirmeyi inanılmaz derecede hızlandıran, tekrarlanabilir ve versiyonlanabilir bir yapı sunar.

## 3. Ollama SDK: Yerel API ile Konuşmak
Ollama servisi arka planda çalışırken, uygulamanızın bu servisle konuşması gerekir. `ollama-python` veya `ollama-js` gibi SDK'lar (Yazılım Geliştirme Kitleri), bu yerel API sunucusuyla iletişim kurmak için basit ve temiz bir arayüz sağlar.

Bu SDK'ların iki temel ve kritik işlevi vardır:

* **Metin Üretimi (Generation):** Bir chat botu veya içerik üretici için modele prompt gönderme.
* **Gömülüler (Embeddings):** Bu, **RAG (Geri Getirme Destekli Üretim)** sistemleri için hayati önem taşır. Ollama, `nomic-embed-text` gibi son teknoloji embedding modellerini de yerel olarak çalıştırabilir. Bu sayede, belgelerinizi vektörlere dönüştürmek için bile buluta veri göndermenize gerek kalmaz.

Bu iki özelliği birleştirdiğinizde, tamamen çevrimdışı çalışan, %100 özel "Belgelerimle Sohbet Et" (Chat with your Docs) uygulamaları oluşturmak mümkün hale gelir.

## 4. Stratejik Kullanım Alanları ve Sınırları
Ollama, bir mühendisin alet çantasındaki yerini hızla almıştır, ancak nerede kullanılacağını bilmek önemlidir.

### Kullanım Alanları:
* **Geliştirme ve Prototipleme:** API maliyetleri veya hız sınırlamaları (rate limiting) olmadan, yerel makinenizde sınırsız ve ücretsiz denemeler yapmanızı sağlar.
* **Gizlilik-Öncelikli Uygulamalar:** Verinin asla cihazdan ayrılmaması gereken sağlık, hukuk veya kişisel finans uygulamaları için tek yoldur.
* **Çevrimdışı (Offline) Yetenekler:** İnternet bağlantısı olmadığında bile çalışması gereken uygulamalar (örn. saha çalışanları için mobil uygulamalar, uçak içi asistanlar).
* **Basit ve Hızlı Görevler:** Karmaşık akıl yürütme gerektirmeyen, ancak anlık yanıt (low latency) beklenen görevler (örn. metin formatlama, basit özetleme).

### Sınırları ve Değiş Tokuş (Trade-offs):
* **Performans:** Ollama, tüketici donanımlarında (özellikle Apple M-serisi) harikalar yaratsa da, alacağınız hız ve performans, buluttaki devasa A100/H100 GPU kümelerinin hızıyla yarışamaz.
* **Model Büyüklüğü:** En güçlü modeller (örn. 70 Milyar parametreli bir Llama 3) çoğu laptop'ta yavaş çalışacaktır. Genellikle daha küçük, kuantize edilmiş 7B veya 8B modellerle (ki bunlar şaşırtıcı derecede yeteneklidir) çalışmak zorunda kalırsınız.
* **Yetenek:** GPT-4o gibi en üst düzey kapalı modellerin karmaşık akıl yürütme yetenekleri, şu an için yerel modellerin önündedir.

Özetle, Ollama "bulutun yerini almaz", ancak onu mükemmel bir şekilde tamamlar. Geliştiricilere, daha önce hiç olmadığı kadar kolay bir şekilde gizlilik, maliyet ve performans arasında seçim yapma özgürlüğü sunar.