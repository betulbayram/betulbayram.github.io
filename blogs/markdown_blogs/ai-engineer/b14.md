# Bölüm 14: RAG (Retrieval-Augmented Generation) - Modelleri Gerçek Dünyaya Bağlamak
Büyük Dil Modelleri (LLM'ler) inanılmaz derecede güçlüdür, ancak iki temel ve yapısal kusurları vardır:

* **Halüsinasyon (Hallucination):** Bilmedikleri veya emin olmadıkları konularda, son derece ikna edici bir dille yanlış bilgi uydurabilirler.
* **Bilgi Kesim Tarihi (Knowledge Cut-off):** Bilgileri, eğitim verilerinin sonlandığı tarihle (örn. Ekim 2023) sınırlıdır. Dünyada o tarihten sonra olan hiçbir şeyi (yeni ürünler, haberler, şirket içi belgeler) "bilemezler".

**Retrieval-Augmented Generation (RAG)**, bu iki sorunu çözmek için tasarlanmış, basit ama son derece etkili bir mimari desendir.

RAG'ın temel fikri şudur: Bir LLM'in bir soruyu "ezberden" yanıtlamasına izin vermek yerine, onu bir **"açık kitap" sınavına** sokmak. Modele, cevaplaması gereken soruyla ilgili doğru bilgileri bir kaynak havuzundan (bilgi tabanınız) bulup getirir ve "Bu soruya, sana verdiğim şu belgelere dayanarak cevap ver" deriz.

Kavramsal olarak RAG, "geri getir ve üret" olmak üzere iki basit adımdan oluşur. Ancak üretim seviyesinde, bu adımların her biri kendi içinde karmaşık alt problemlere ve optimizasyon fırsatlarına sahiptir.

## RAG Yaşam Döngüsü: Prototipten Üretime
Bir RAG sistemi kurmak, iki ayrı aşamadan oluşur: **İndeksleme** (Veriyi Hazırlama - Çevrimdışı) ve **Çıkarım** (Sorgu Anı - Çevrimiçi).

### Aşama 1: İndeksleme (Veri Hazırlığı)
Bu, modelinizin "açık kitap" sınavı için çalışacağı "ders notlarını" hazırlama sürecidir. RAG'ın performansı, bu aşamada hazırlanan bağlamın (context) kalitesiyle doğrudan ilişkilidir.

#### 1. Yükleme ve Parçalama (Load & Chunk)
İlk olarak, bilgi tabanınızı (PDF'ler, web siteleri, Notlar, Confluence) yüklersiniz. Bu belgeler, LLM'lerin bağlam penceresine sığması ve arama için verimli olması amacıyla daha küçük, yönetilebilir **parçalara (chunks)** bölünür.

Ancak bu "parçalama" (chunking) stratejisi, RAG performansını doğrudan etkileyen en kritik kararlardan biridir:

* **Naif Yaklaşım (Fixed-Size):** Metni körü körüne 500 token'lık parçalara bölmek.
    * **Problem:** Bu, bir cümlenin veya fikrin tam ortasında kesilmelere yol açar. Bir parçanın sonu ile diğerinin başı arasındaki anlamsal bağ kopar.
* **Gelişmiş Yaklaşım (RecursiveCharacterTextSplitter):** LangChain gibi framework'lerin popüler hale getirdiği bu yöntem, metni önce paragraflara (`\n\n`), sonra satırlara (`\n`), sonra boşluklara (` `) göre bölmeye çalışarak anlamsal bütünlüğü bir nebze korumaya çalışır. Bu "yeterince iyi" bir başlangıçtır.
* **Üretim Seviyesi Yaklaşım (Semantic Chunking):**
    * Burada metin, sabit karakter sayısına göre değil, anlamsal kaymalara göre bölünür. Paragraflar veya cümleler bir embedding modelinden geçirilir ve ardışık parçalar arasındaki anlamsal benzerlik skoru hesaplanır.
    * Benzerlik skoru belirli bir eşiğin altına düştüğünde (yani konu değiştiğinde), bu, yeni bir "chunk" için mükemmel bir sınırdır. Bu, her parçanın kendi içinde tutarlı bir konuyu barındırmasını sağlar.
* **Parça Kesişimi (Chunk Overlap):** Parçalarınızı (örn. 500 token) 50 token'lık bir kesişme payıyla (overlap) oluşturmak bir standarttır. Bunun nedeni, bir sorgunun cevabının tam olarak iki parçanın sınırına denk gelmesi durumunda, bu "kesişim" bölgesinin her iki parçada da bulunarak geri getirilme şansını artırmasıdır.

#### 2. Gömme (Embedding)
Her bir "chunk", (Bölüm 12'de tartıştığımız) bir embedding modeli (örn. `bge-base` veya OpenAI API) kullanılarak anlamsal bir vektöre dönüştürülür. Artık her metin parçasının, onun anlamsal özünü temsil eden bir sayı dizisi (vektör) karşılığı vardır.

#### 3. Depolama (Storage)
Bu vektörler ve onlara karşılık gelen orijinal metin parçaları, (Bölüm 13'te gördüğümüz) bir **Vektör Veritabanına** (örn. `Chroma`, `Pinecone`, `pgvector`) yüklenir. Veritabanı, bu milyonlarca vektör arasında hızlı anlamsal arama yapmak için bir ANN (Yaklaşık En Yakın Komşu) indeksi oluşturur. "Hafıza" artık hazırdır.

---

### Aşama 2: Çıkarım (Sorgu ve Üretim)
Bu, kullanıcı sistemle her etkileşime girdiğinde milisaniyeler içinde gerçekleşen süreçtir.

#### 4. Geri Getirme (Retrieval)
Kullanıcı bir sorgu ("Yeni Z Raporu'ndaki temel bulgular nelerdir?") gönderir. Bu sorgu, indekslemede kullanılan modelin **aynısı ile** bir vektöre dönüştürülür. Vektör veritabanından bu sorgu vektörüne anlamsal olarak en yakın **Top-K** (örn. en yakın 5) belge parçası (chunk) geri çağrılır.

Ancak, üretim seviyesinde sadece Top-K'ya güvenmek iki temel soruna yol açar:

* **Anlamsal Tekrarcılık:** En yakın 5 parça, sorguyla ilgili olabilir ancak hepsi aynı şeyin farklı ifadeleri olabilir. Bu, LLM'in bağlam penceresini (context window) gereksiz yere doldurur.
* **Anahtar Kelime Zayıflığı:** Vektörel arama, "anlam" konusunda harikadır ancak "belirli kodlar" (SKU, ürün ID'si, yasal referans) veya "isimler" konusunda zayıftır. Kullanıcı "Rapor XYZ-2024" diye arattığında, vektör motoru "2024 yılına ait finansal belgeler" gibi anlamsal olarak benzer ama yanlış sonuçlar getirebilir.

**Gelişmiş Geri Getirme Çözümleri:**

* **Maximal Marginal Relevance (MMR):** Bu, sadece "benzerlik" değil, aynı zamanda "çeşitlilik" (diversity) için de optimizasyon yapan bir algoritmadır. Önce sorguya en uygun parçayı bulur, ardından hem sorguya uygun olan hem de daha önce seçilen parçalardan farklı olan bir sonraki parçayı seçer. Bu, bağlama farklı perspektifler getirir.
* **Hybrid Search (Melez Arama):** Üretim seviyesi RAG'ın altın standardıdır. Bu yöntem, iki arama dünyasının en iyilerini birleştirir:
    * **Vektör Araması (Anlamsal):** Sorgunun anlamını ve niyetini yakalar.
    * **Keyword Arama (BM25/TF-IDF):** Geleneksel, anahtar kelime tabanlı arama. Belirli isimleri, kodları ve terimleri kaçırmaz.
    * Bu iki arama motoru paralel çalıştırılır ve sonuçları birleştirmek için **Reciprocal Rank Fusion (RRF)** gibi algoritmalar kullanılır. RRF, her iki listenin sıralamasına bakarak en tutarlı sonuçları zirveye taşır.

#### 5. Üretim (Generation)
Bu, mimarinin "Augmented" (Desteklenmiş) kısmıdır. Geri getirilen (ve ideal olarak yeniden sıralanan) parçalar, kullanıcının orijinal sorgusuyla birlikte bir LLM'e (örn. GPT-4, Llama 3) gönderilir.

Modele gönderilen prompt (talimat) aslında şöyle bir şeye benzer:
> **Sistem Talimatı:** Sen, yalnızca sana sağlanan BAĞLAM'a dayanarak cevap veren bir asistansın.
>
> **BAĞLAM:**
> [Geri getirilen 1. belge parçası buraya...]
> [Geri getirilen 2. belge parçası buraya...]
> ...
>
> **SORU:** Yeni Z Raporu'ndaki temel bulgular nelerdir?
>
> **Cevap (Modelin üreteceği):**

Ancak, bu noktada kritik bir sorunla karşılaşırız: **"Lost in the Middle" (Ortada Kaybolma)**. Yapılan araştırmalar, LLM'lerin bir prompt'a yerleştirilen bağlamın en başına ve en sonuna daha fazla dikkat ettiğini, ortadaki bilgilere ise daha az ağırlık verdiğini göstermiştir.

**Çözüm: Yeniden Sıralama (Re-ranking) Aşaması:** Bu, iki aşamalı bir geri getirme sürecidir.
* **Aşama 1 (Hızlı Retrieval):** Vektör veritabanından `Top-50` gibi geniş bir aday listesi çekilir. Bu hızlıdır ancak "gürültülü" olabilir.
* **Aşama 2 (Yeniden Sıralama):** Bu 50 aday, `Cohere Rerank` gibi daha küçük, özel bir yeniden sıralama modeline veya bir **Cross-Encoder**'a (ki bu, S-BERT'ten daha yavaştır ancak daha doğrudur) verilir. Bu model, 50 parçayı "sorgu ve parça" çiftleri olarak derinlemesine analiz eder ve LLM'e gönderilecek en iyi `Top-5` parçayı seçer.

Bu yöntem, LLM'in bağlam penceresine giren bilginin mümkün olan en yüksek kalitede olmasını sağlar ve "ortada kaybolma" riskini azaltır.

---

## Stratejik Kararlar: RAG, Fine-Tuning ve Uygulama

### RAG vs. Fine-Tuning (İnce Ayar)
Bu, mühendislikte en sık karıştırılan konulardan biridir. İkisi birbirinin rakibi değil, farklı sorunları çözen müttefiklerdir.

* **Fine-Tuning:** Modelin **davranışını, stilini veya formatını** öğretmek için kullanılır. Modelin dolaylı bilgisini (implicit knowledge) değiştirir.
    * **Ne Zaman Kullanılır:** Modelinizin her zaman JSON formatında çıktı vermesini, belirli bir marka sesiyle konuşmasını veya "hukuk" gibi belirli bir alanın jargonunu daha iyi anlamasını istiyorsanız.
* **RAG:** Modele **olgusal (factual), dinamik ve spesifik bilgi** sağlamak için kullanılır. Modelin çekirdek bilgisi değişmez, sadece ona "kopya kağıdı" verilir.
    * **Ne Zaman Kullanılır:** Modelin dünkü satış rakamlarını, İK politikalarınızı veya en son haberi bilmesini istiyorsanız.

> **Altın Kural:** Becerileri öğretmek için fine-tuning yapın, bilgileri sağlamak için RAG kullanın.

Ancak üretim seviyesinde, cevap genellikle **"Her ikisi de"**dir:

1.  **Embedding Modelini Fine-Tune Etmek:** Kullandığınız `bge-base` gibi genel amaçlı embedding modelleri, sizin şirketinizin özel jargonu (örn. "Proje Phoenix") hakkında hiçbir fikre sahip değildir. Bu durumda, embedding modelinin kendisini kendi verilerinizle fine-tune etmek, geri getirme (retrieval) kalitesini dramatik şekilde artırır.
2.  **LLM'i (Generator) Fine-Tune Etmek:** RAG sisteminizden gelen (Bağlam, Sorgu, İdeal Cevap) binlerce örnek oluşturursunuz. Daha sonra, kullandığınız LLM'i (örn. Llama 3) bu verilerle fine-tune edersiniz. **Amaç:** LLM'e bilgi öğretmek değil, LLM'e sağlanan bağlamı nasıl daha iyi sentezleyeceğini, bağlamdaki çelişkili bilgileri nasıl ele alacağını veya bağlam yetersiz olduğunda nasıl 'bilmiyorum' diyeceğini öğretmektir.

### Uygulama Ekosistemi ve "Abstraction Vergisi"
Bu RAG akışını sıfırdan, saf SDK'ları (OpenAI SDK, Supabase SDK vb.) kullanarak kendiniz de yazabilirsiniz. Ancak bu çok fazla "tesisat kodu" (boilerplate code) gerektirir.

Bu süreci basitleştirmek ve orkestre etmek için güçlü framework'ler (çatılar) ortaya çıkmıştır:

* **LangChain:** RAG ve Agent (Ajan) uygulamaları geliştirmek için en popüler ve kapsamlı framework'tür. Modüler "zincirler" (Chains) halinde tüm RAG adımlarını birleştirmenizi sağlar.
* **LlamaIndex:** Başlangıçta özellikle RAG'ın "veri" kısmına (indeksleme, geri getirme) odaklanan bir veri framework'üdür. Gelişmiş indeksleme ve geri getirme stratejileri konusunda çok güçlüdür.
* **Haystack & RAGFlow:** Ekosistemdeki diğer güçlü oyunculardır; özellikle Haystack (Deepset'ten), esnek pipeline'lar kurma konusunda olgun ve kurumsal kullanıma hazır bir yapı sunar.

**"Abstraction Vergisi" (Abstraction Tax) ve Üretim Gerçeği:**
Bu framework'ler hızlı prototipleme için harikadır. Ancak, içlerindeki "sihirli" (magic) `Chains` ve `Agents` yapıları, üretim ortamında hata ayıklamayı (debugging) zorlaştırabilir. Çoğu tecrübeli ekip, sistemi anlamak için bu framework'lerle başlar, ancak daha sonra tam kontrol, şeffaflık ve performans elde etmek için kendi pipeline'larını saf SDK'ları (`FastAPI` + `sentence-transformers` + `Ollama/OpenAI SDK` + `Pinecone SDK`) kullanarak yazmaya yönelir.