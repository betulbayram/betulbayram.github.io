## Bölüm 4: Transfer Öğrenmesi ve İnce Ayar (Fine-Tuning) Sanatı
Önceki bölümde, internet ölçeğinde verilerle eğitilmiş devasa ön-eğitimli modelleri ele aldık. Bu modeller, dili veya görselleri genel bir seviyede anlama konusunda muazzam bir yeteneğe sahipler. Ancak, bu genel bilgi, bizim spesifik iş problemimizi (örneğin, "sadece bizim şirketimizin hukuki belgelerini sınıflandırmak" veya "sadece fabrikamızdaki kusurlu ürünleri tespit etmek") doğrudan çözmez.

İşte bu noktada **Transfer Öğrenmesi (Transfer Learning)** devreye girer.

Transfer Öğrenmesi, bir alanda (kaynak alan) edinilen bilginin, farklı ama ilişkili bir alandaki (hedef alan) bir problemi çözmek için yeniden kullanılmasıdır. **İnce Ayar (Fine-Tuning)** ise bu felsefenin en yaygın ve güçlü teknik uygulamasıdır.

Bu süreci, yıllarca genel tıp eğitimi almış bir doktorun, belirli bir alanda (örneğin *kardiyoloji*) uzmanlaşması için aldığı ihtisas eğitimine benzetebiliriz. Doktor, tıp hakkındaki temel bilgilerini (*genel anatomi, biyokimya*) unutmaz, ancak bu bilgiyi yeni ve spesifik bir alanda derinleştirir.

### 1. Mekanizma: Beden (Body) ve Baş (Head)
Bir ön-eğitimli modeli (örn. *ResNet* veya *BERT*) iki parça olarak düşünebiliriz:

-   **Beden (Body/Backbone):** Modelin alt ve orta katmanlarıdır. Bu katmanlar, verinin genel, düşük ve orta seviyeli **temsillerini (representations)** öğrenir. Görsel için bu, kenarlar, dokular, şekillerdir. Dil için bu, kelime anlamları, gramer kuralları ve sentaktik ilişkilerdir. Modelin "bilgeliği" bu kısımda saklıdır.
-   **Baş (Head):** Modelin son bir veya birkaç katmanıdır. Bu katman, "Beden"den gelen zengin temsilleri alır ve bunları orijinal göreve (örneğin, ImageNet'in 1000 sınıfı veya BERT'in "maskeli kelimeyi tahmin etme") özel bir çıktıya dönüştürür.

Fine-tuning yaparken, modelin orijinal "Baş" kısmını atarız. Çünkü bizim 1000 sınıflı bir sınıflandırıcıya ihtiyacımız yoktur. Onun yerine, kendi görevimize uygun (örneğin, 2 sınıflı *"olumlu/olumsuz"* sınıflandırıcısı) yeni, rastgele başlatılmış bir "Baş" ekleriz.

Asıl sanat, bu yeni "Baş"ı ve "Beden"in ne kadarını, nasıl eğiteceğimizde yatar.

### 2. İnce Ayar Stratejileri: Dondurma, Çözme ve Öğrenme Oranları
Amacımız, modelin genel bilgisini korurken yeni görevimize adapte olmaktır. Bunun için birkaç temel strateji vardır:

#### Strateji 1: Sadece "Baş"ı Eğitmek (Hızlı ve Güvenli)
Bu, en basit transfer öğrenmesi yöntemidir. Modelin tüm "Beden" katmanlarını **dondururuz** (parametrelerinin güncellenmesini engelleriz). Sadece kendi eklediğimiz yeni "Baş" katmanını eğitiriz.

-   **Nasıl:** `layer.trainable = False`
-   **Avantajı:** Çok hızlıdır, düşük donanım (VRAM) gerektirir ve çok az veriyle bile çalışır. Modelin genel bilgisi hiç bozulmaz.
-   **Dezavantajı:** Performans sınırlıdır, çünkü modelin genel özellikleri bizim görevimiz için mükemmel olmayabilir.
-   **Ne Zaman:** Hızlı bir temel model (baseline) oluşturmak, veri setinin çok küçük olduğu veya ön-eğitimli modelin görevine çok benzer olduğu durumlarda kullanılır.

#### Strateji 2: Aşamalı "Çözme" (Gradual Unfreezing) (En Etkili Yöntem)
Bu, en dengeli ve yüksek performanslı sonuçları veren stratejidir.

1.  **Aşama 1:** Önce, Strateji 1'deki gibi tüm "Beden"i dondurup sadece yeni "Baş"ı birkaç epoch (tur) eğitiriz. Bu, yeni "Baş" katmanının rastgele ağırlıklardan kurtulup anlamlı bir hale gelmesini sağlar.
2.  **Aşama 2:** Ardından, "Beden"in en üstteki birkaç katmanını (örneğin son bloğunu) "çözeriz" (`layer.trainable = True`).
3.  **Aşama 3:** Tüm modeli (çözülmüş katmanlar + baş) **çok düşük bir öğrenme oranı (learning rate)** ile tekrar eğitiriz.

**Neden bu işe yarar?** Çünkü modelin ilk katmanları (kenarlar, çizgiler gibi) neredeyse evrenseldir ve değiştirilmemelidir. Son katmanlar ise (karmaşık şekiller, desenler) göreve daha özeldir ve bizim problemimize uyum sağlamaları için hafifçe güncellenmeleri gerekir.

#### Strateji 3: Tüm Modeli Fine-Tune Etmek (Riskli ama Güçlü)
Yeterli verimiz ve zamanımız varsa, tüm modeli (Beden + Baş) en baştan itibaren, ancak yine de çok düşük bir öğrenme oranıyla eğitebiliriz.

-   **Avantajı:** Potansiyel olarak en yüksek doğruluğu verir.
-   **Dezavantajı:** Yüksek donanım gerektirir ve en büyük risk olan **Felaket Unutma (Catastrophic Forgetting)** tehlikesini taşır.

### 3. Kritik Zorluklar ve Çözümleri
İnce ayar süreci, dikkatle yönetilmesi gereken hassas bir dengedir.

#### Zorluk 1: Felaket Unutma (Catastrophic Forgetting)
-   **Problem:** Modeli yeni görevimizle (ve genellikle küçük olan veri setimizle) agresif bir şekilde eğitirsek, model genel ("milyonlarca resimden öğrendiği") bilgisini "unutur" ve sadece bizim küçük veri setimizi ezberlemeye başlar (*overfitting*).
-   **Çözüm: Ayrımsal Öğrenme Oranları (Discriminative Learning Rates).** Bu, aşamalı çözmenin bir uzantısıdır. "Baş" katmanına yüksek bir öğrenme oranı (örn. `1e-3`), "Beden"in üst katmanlarına daha düşük bir oran (örn. `1e-4`) ve "Beden"in alt katmanlarına çok düşük bir oran (örn. `1e-5`) veririz. Böylece modelin temel bilgisi çok az değişirken, göreve özel katmanlar daha hızlı adapte olur.

#### Zorluk 2: Alan Uyuşmazlığı (Domain Mismatch)
-   **Problem:** Genel internet metinleriyle (Wikipedia) eğitilmiş bir BERT modelini, tıp makaleleri (PubMed) veya hukuk metinleri üzerinde fine-tune etmeye çalışmak. Model, alana özgü jargonu, terminolojiyi ve anlam ilişkilerini bilmez.
-   **Çözüm:** İki aşamalı bir yaklaşım gerekir.
    1.  **Domain-Adaptive Pre-training (DAPT):** Modeli alır, önce hedef alanımızdaki (örn. binlerce tıbbi makale) etiketsiz verilerle bir süre daha "ön-eğitim" (maskeli kelime tahmini gibi) yapmaya devam ederiz. Bu, modelin tıbbi jargonu öğrenmesini sağlar.
    2.  **Fine-Tuning:** Alan dilini öğrenen bu modeli, sonra kendi küçük etiketli veri setimizle (örn. "tümör var/yok") fine-tune ederiz. Bu yaklaşım, performansı dramatik olarak artırır.

#### Zorluk 3: Veri Kalitesi
-   **Problem:** Ön-eğitimli model ne kadar güçlü olursa olsun, fine-tuning veriniz (etiketleriniz) kirliyse, model o kirliliği ezberler.
-   **Çözüm:** Modelin gücüne güvenmek yerine, zamanın önemli bir kısmını fine-tuning için kullanılacak o "küçük ama değerli" veri setini temizlemeye ve doğrulamaya harcamak gerekir. Az miktarda yüksek kaliteli veri, çok miktarda gürültülü veriden her zaman daha iyidir.

Özetle, "fine-tuning" sadece `model.fit()` komutunu çalıştırmak değildir. Bu, hangi katmanların dondurulacağına, hangi öğrenme oranlarının kullanılacağına ve modelin eski bilgisi ile yeni görevi arasındaki hassas dengenin nasıl korunacağına dair verilen stratejik bir mühendislik kararları dizisidir.