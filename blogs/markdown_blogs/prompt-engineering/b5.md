## Bölüm 5: Yapılandırılmış Çıktılar - LLM'i "Demo"dan "Üretime" Taşımak
Bir LLM'i bir API'nin arkasına koyduğunuzda, en büyük kâbusunuz öngörülemezliktir. Eğer uygulamanızın bir sonraki adımı, modelin cevabına göre (örn. bir `user_id` alıp veritabanından sorgu yapmak) çalışacaksa, o cevabın her zaman beklenen formatta gelmesi gerekir.

Serbest metin (string) bir demo içindir. **Yapılandırılmış veri (JSON, XML, YAML)** ise **üretim (production)** içindir. İşte bu dönüşümü zorlamanın (enforce) yolları, en az güvenilirden en sağlam olanına doğru.

### 1. Yöntem: "Umut Mühendisliği" (Prompt-Based Control)
Bu, modelin davranışını sadece prompt (talimat) ile yönlendirmeye çalıştığımız "yumuşak" (soft) yöntemlerdir.

#### a) Zero-Shot Talimatı (En Zayıf Yöntem):
**Prompt:** "Bana kullanıcının adını ve yaşını JSON olarak ver."

**Problem:** Bu, en güvenilmez yöntemdir. Model, 10 denemenin 9'unda size şöyle bir cevap verebilir:
> "Tabii, işte istediğiniz JSON:
> \```json
> {"name": "Ahmet", "age": 30}
> \```
> Umarız yardımcı olmuştur!"

Bu çıktı, bir `JSON.parse()` fonksiyonu tarafından doğrudan ayrıştırılamaz. Öncesindeki ve sonrasındaki metni (preamble/postamble) temizlemek için karmaşık ve kırılgan regex (regular expressions) kodları yazmanız gerekir.

#### b) Few-Shot Örneklemesi (Daha İyi ama Kırılgan):
**Prompt:** Bölüm 1'de gördüğümüz gibi, modele bir örnek verirsiniz.
> "Metin: 'Ali 25 yaşında.' -> {"name": "Ali", "age": 25}
>
> Metin: 'Veli 40 yaşında.' ->"

**Problem:** Bu, güvenilirliği artırır ancak garanti etmez. Model, formatı taklit etmeyi öğrenir ancak yine de fazladan metin ekleyebilir veya karmaşık girdilerde formatı bozabilir. Ayrıca, bu örnekler değerli bağlam pencerenizi (context window) tüketir.

---

### 2. Yöntem: "Zorlanmış" Kontrol (Enforced Control) - Gerçek Mühendislik
Bu yöntemler, modelin "hata yapma" şansını ortadan kaldırır. Modelin serbest metin üretmesine izin vermez; onu sadece istediğimiz formatta çıktı üretmeye zorlarız.

#### a) Function Calling / Tool Use (API Tabanlı Modeller)
Bu, OpenAI, Google (Gemini) ve Anthropic (Claude) gibi API sağlayıcılarının bu soruna bulduğu en güçlü ve zarif çözümdür.

**Teknik olarak nasıl çalışır:**
1.  API çağrısı yaparken, modele sadece prompt'u değil, aynı zamanda **`tools` (araçlar)** veya **`functions` (fonksiyonlar)** adlı bir parametre gönderirsiniz.
2.  Bu parametre, modelin çağırabileceği fonksiyonların bir listesini ve her birinin gerektirdiği argümanları tanımlayan bir **JSON Schema** içerir.
    > **Örnek Şema:**
    > ```json
    > {"name": "kullanici_kaydet", "parameters": {"type": "object", "properties": {"ad": {"type": "string"}, "yas": {"type": "number"}}, "required": ["ad", "yas"]}}
    > ```
3.  Model, kullanıcının prompt'unu ("Yeni kullanıcı Ahmet, 30 yaşında") alır.
4.  Cevap olarak serbest bir metin üretmek yerine, özel olarak bu iş için eğitildiği için, `kullanici_kaydet` aracını çağırması gerektiğine karar verir.
5.  API yanıtı olarak bir metin dönmez. Bunun yerine, şu özel `tool_call` nesnesini döndürür:
    > ```json
    > "tool_calls": [{"function": {"name": "kullanici_kaydet", "arguments": "{\"ad\": \"Ahmet\", \"yas\": 30}"}}]
    > ```

**Neden Devrimsel:** Bu bir "prompt hilesi" değildir. Modelin doğal bir yeteneğidir. Çıktının, sağladığınız şemaya %100 uyan, garantili ve ayrıştırılabilir bir JSON `arguments` dizesi olmasını sağlar. Bu, Bölüm 15'te gördüğümüz AI Ajanlarının temel taşıdır.

#### b) JSON Mode (Modern API'ler)
Function Calling'in biraz daha basit bir versiyonudur. OpenAI gibi sağlayıcılar artık `response_format={ "type": "json_object" }` gibi bir parametre sunuyor.

* **Nasıl Çalışır:** Modele yine prompt'ta JSON üretmesini söylemeniz gerekir. Ancak bu bayrak (flag), modelin çıktısının sözdizimsel olarak (syntactically) geçerli bir JSON nesnesi olacağını garanti eder.
* **Farkı:** Function Calling'in aksine, bu mod genellikle belirli bir şemayı (schema) zorlamaz; sadece çıktının genel bir JSON olmasını sağlar.

#### c) Dilbilgisi Tabanlı Örnekleme (Grammar-Based Sampling) (Açık Kaynak Gücü)
Bu, açık kaynak modeller (Llama, Mistral) dünyasındaki en güçlü ve teknik yöntemdir. `Llama.cpp`, `vLLM` gibi inference (çıkarım) sunucularında veya `outlines`, `guidance` gibi kütüphanelerde uygulanır.

**Teknik olarak nasıl çalışır:** Bu yöntem, Bölüm 3'te gördüğümüz Sampling (Örnekleme) sürecine müdahale eder.
1.  Siz, bir Pydantic modeli, bir regex veya bir JSON şeması gibi bir **"dilbilgisi" (grammar)** tanımlarsınız.
2.  LLM, bir sonraki token'ı üretmek için olasılık dağılımını (binlerce token'lık liste) oluşturur.
3.  "Dilbilgisi filtresi" bu listeyi alır ve o anki (örneğin `{"name": "A`) duruma göre geçerli olmayan tüm token'ların olasılığını sıfıra ayarlar (logit bias/mask).
4.  **Örnek:** Model `{"name": "` metnini yeni üretti. Dilbilgisi filtresi, bir sonraki geçerli token'ın sadece bir dize başlatıcısı (`"`) olabileceğini bilir. Modelin `123` (sayı) veya `}` (kapatma) gibi token'ları seçme olasılığını tamamen ortadan kaldırır.
5.  Model, geriye kalan bu "güvenli" ve "geçerli" token'lar arasından (Temperature, Top-P vb. kullanarak) bir seçim yapmak zorunda kalır.

**Sonuç:** Çıktının, tanımladığınız dilbilgisine (şemaya) %100 uyması garanti edilir. Bu, modelin "isteyip istememesi" ile ilgili değildir; modelin başka bir seçeneği yoktur.

---

### Özetle:
Bir mühendis olarak, yapılandırılmış çıktı için "umut" (prompting) ile "garanti" (zorlama) arasındaki farkı bilmeliyiz:

| Yöntem | Kullanım Alanı | Güvenilirlik | Teknik |
| :--- | :--- | :--- | :--- |
| **Zero/Few-Shot** | Hızlı prototipleme, düşük riskli görevler. | Düşük / Orta | Prompt Engineering (Yumuşak) |
| **Function Calling** | Üretim (API Tabanlı), Ajanlar, Araç Kullanımı. | **Çok Yüksek (Garanti)** | Model Yeteneği (Zorlama) |
| **JSON Mode** | API'lerden basit, şemasız JSON alma. | Yüksek (Sözdizimi Garanti) | Model Yeteneği (Zorlama) |
| **Grammar-Based** | Üretim (Açık Kaynak), tam şema zorlaması. | **Çok Yüksek (Garanti)** | Inference Müdahalesi (Zorlama) |

**Serbest metinle uğraşmayı bırakıp `Function Calling` veya `Grammar-Based Sampling` kullanmaya başladığınız an, bir "demo" yapmaktan çıkıp "ürün" geliştirmeye başladığınız andır.**